{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed to run this app\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from datetime import time, datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depreciated way of opening ChromeDriver\n",
    "\n",
    "# !which chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open ChromeDriver\n",
    "\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to the disired url\n",
    "\n",
    "base_url = 'https://www.walmart.com'\n",
    "search_url = '/search/?query=room%20air%20purifier'\n",
    "browser.visit(base_url + search_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the html and parse it\n",
    "\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the product scrape\n",
    "\n",
    "# Create empty lists for scraped data to be stored in.\n",
    "productURL_list = []\n",
    "productImage_list = []\n",
    "productTitle_list = []\n",
    "starReview_list = []\n",
    "reviewAmount_list = []\n",
    "currentPrice_list = []\n",
    "reviewURL_list = []\n",
    "freepickup_list = []\n",
    "\n",
    "# Find the number of pages for this site\n",
    "listOfPages = soup.find('ul', class_='paginator-list').find_all('li')\n",
    "numberOfPages = listOfPages[0].find('a')['aria-label'].split()[3]\n",
    "print(f\"the number of pages to loop through is {numberOfPages}\")\n",
    "\n",
    "# datetime object containing current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "for i in range(1,int(numberOfPages)+1):\n",
    "# for i in range(1,2):\n",
    "    \n",
    "    # click on the next page number and scrape the html\n",
    "    page_link = browser.links.find_by_text(i).click()\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # loop through all of the products on that page\n",
    "    products = soup.find_all('div', class_='search-result-gridview-item')\n",
    "    print(f\"quering page {i}...............\")\n",
    "\n",
    "    for product in products:\n",
    "\n",
    "# Get the link to the product page.\n",
    "        link = product.find('a')\n",
    "        href = link['href']\n",
    "        product_url = base_url + href\n",
    "        productURL_list.append(product_url)\n",
    "\n",
    "# Get the link to the product image.\n",
    "        img = product.find('img')['src']\n",
    "        productImage_list.append(img)\n",
    "\n",
    "# Get the product title.\n",
    "        product_title = product.find('img')['alt']\n",
    "        productTitle_list.append(product_title)\n",
    "\n",
    "# Get the number of reviews, there may not be any.\n",
    "        try:\n",
    "            review_amount = product.find('span', class_='seo-review-count visuallyhidden').text\n",
    "            reviewAmount_list.append(review_amount)\n",
    "\n",
    "        except:\n",
    "            review_amount = 0\n",
    "            reviewAmount_list.append(review_amount)\n",
    "\n",
    "        # In the case where there are at least one review...\n",
    "        if int(review_amount) > 0:\n",
    "            # Get the average number of stars\n",
    "            stars_review = product.find('span', class_='visuallyhidden seo-avg-rating').text   \n",
    "            starReview_list.append(stars_review)\n",
    "\n",
    "            # Get the URL to the reviews section for that product.\n",
    "            review_url = product.find('div', class_='stars').find('a')['href']\n",
    "            reviewURL_list.append(base_url+review_url)\n",
    "\n",
    "        # Otherwise, use defalt 0 or NaN values for these entries.\n",
    "        else:\n",
    "            stars_review = 0\n",
    "            starReview_list.append(stars_review)\n",
    "\n",
    "            review_url = \"NaN\"\n",
    "            reviewURL_list.append(review_url)\n",
    "\n",
    "# Get the price of the product.\n",
    "        try:\n",
    "            price = product.find('span', class_='price-main-block')\n",
    "            current_price = price.find('span', class_='visuallyhidden').text\n",
    "        # Some products don't have a specific price, or price is only shown in cart\n",
    "        except:\n",
    "            price = product.find('span', class_='search-result-productprice')\n",
    "            current_price = price.find('span', class_='visuallyhidden').text\n",
    "        \n",
    "        currentPrice_list.append(current_price)\n",
    "        \n",
    "# Check to see if the product has 'Free pickup' AKA in store\n",
    "        # Look at the div under the price div\n",
    "        try:\n",
    "            shipping_details = product.find('div', class_='search-result-product-shipping-details')\n",
    "            delivery_options_list = []\n",
    "            # loop through all possile delivery options displayed\n",
    "            for option in shipping_details.children:\n",
    "                option.span.unwrap() #unwrap takes the span tag off\n",
    "                delivery_option = option.text\n",
    "                if delivery_option == 'Free pickup': #assuming 'free pickup' means avaliable in store\n",
    "                    free_pickup = True\n",
    "                    delivery_options_list.append(free_pickup)\n",
    "                else:\n",
    "                    free_pickup = False\n",
    "                    delivery_options_list.append(free_pickup)\n",
    "\n",
    "            instore = any(delivery_options_list)\n",
    "        # if there are no delivery options, assume not avaialble in store\n",
    "        except:\n",
    "            instore = False\n",
    "        freepickup_list.append(instore)\n",
    "\n",
    "    # Close the browser window\n",
    "    # browser.quit()\n",
    "\n",
    "    # Create a dictionary with the lists of the scrapped data.\n",
    "    data = {\n",
    "        \"Title\": productTitle_list,\n",
    "        \"URL\": productURL_list,\n",
    "        \"Image\": productImage_list,\n",
    "        \"AverageStars\": starReview_list,\n",
    "        \"NumberofReviews\": reviewAmount_list,\n",
    "        \"ReviewsURL\": reviewURL_list,\n",
    "        \"Price\": currentPrice_list,\n",
    "        \"Free Pickup\": freepickup_list\n",
    "           }\n",
    "\n",
    "    # Create a Pandas DataFrame with that dictionary\n",
    "    product_df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting columns as necessary and checking the data types\n",
    "\n",
    "product_df = product_df.astype({\n",
    "    \"NumberofReviews\": 'int',\n",
    "    \"AverageStars\": 'float'\n",
    "})\n",
    "\n",
    "product_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering dataframe before saving as csv\n",
    "\n",
    "ordered_df = product_df.sort_values(by=['Free Pickup', 'NumberofReviews'], ascending=False)\n",
    "ordered_df.head()\n",
    "# shortlist_df = product_df.sort_values(by='NumberofReviews', ascending=False)[product_df['Free Pickup']==True]\n",
    "# shortlist_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking the length of the dataframe and other stats\n",
    "\n",
    "# product_df.describe()\n",
    "# product_df.head(6)\n",
    "ordered_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding datetime of scrape to csv name\n",
    "# dd/mm/YY H:M:S\n",
    "\n",
    "dt_string = now.strftime(\"%d_%m_%Y__%H_%M_%S\")\n",
    "ordered_df.to_csv(f\"WalmartRAPScrape_{dt_string}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Scrape Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Testing Comments Scrape Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(product_df['URL'][0])\n",
    "print('----------------------------------------------------------------------------------------------------')\n",
    "print(product_df['ReviewsURL'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT OFF HERE\n",
    "# WANT TO CLICK BY TEXT LIKE IN ABOVE QUERY, DIDN'T WORK THOUGH...\n",
    "# THIS WILL LET ME GO TO EACH PAGE OF THE REVIEWS, \n",
    "# AND I THINK IT WILL LET ME LOAD ALL OF THAT PAGES REVIEWS\n",
    "\n",
    "# Get reviews \n",
    "\n",
    "review_df = pd.DataFrame()\n",
    "\n",
    "# loop through all of the products in the product dataframe\n",
    "# for i in range(len(product_df[\"ReviewsURL\"])):\n",
    "for i in range(4,6):\n",
    "    if product_df['NumberofReviews'][i] > 0:\n",
    "        \n",
    "        productTitleList = []\n",
    "        reviewURLList = []\n",
    "        reviewTitleList = []\n",
    "        reviewRatingList = []\n",
    "        reviewCommentList = []\n",
    "    \n",
    "#         try:\n",
    "        browser.visit(product_df[\"ReviewsURL\"][i])\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        allReviewsURL = soup.find('a', class_=\"button ReviewBtn-container ReviewsHeader-seeAll button--primary\")['href']\n",
    "        browser.visit(base_url+allReviewsURL)\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # loop through all of the pages in this search\n",
    "        reviewPages = soup.find('ul', class_='paginator-list').find_all('li')\n",
    "        pagesOfReviews = reviewPages[-1].find('button').text\n",
    "        print(f\"the number of pages to loop through is {pagesOfReviews}\")\n",
    "        for j in range(1, int(pagesOfReviews)+1):\n",
    "            \n",
    "            page_link = browser.links.find_by_text(j).click()\n",
    "            html = browser.html\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            reviews = soup.find_all('div', class_='ReviewList-content')\n",
    "\n",
    "            for review in reviews:\n",
    "\n",
    "                productTitleList.append(product_df['Title'][i])\n",
    "                reviewURLList.append(product_df['ReviewsURL'][i])\n",
    "\n",
    "                try:\n",
    "                    title = review.find('h3', class_='review-title').text\n",
    "                    reviewTitleList.append(title)\n",
    "                    print(title)\n",
    "\n",
    "                except:\n",
    "                    title = 'None'\n",
    "                    reviewTitleList.append(title)\n",
    "                    print(title)\n",
    "#                         pass\n",
    "\n",
    "#                 # adding try only because the browser needs to scroll\n",
    "#                 # once this is added, there wont need to be a try\n",
    "#                 # each review has to have a star value\n",
    "#                 try:\n",
    "#                     reviewRating = review.find('span', class_='seo-avg-rating').text\n",
    "#                     reviewRatingList.append(reviewRating)\n",
    "#                     print(reviewRating)\n",
    "#                 except:\n",
    "#                     pass\n",
    "\n",
    "#                 # adding try and except for the review body for same reason as stars\n",
    "#                 try:\n",
    "#                     reviewComment = review.find('p').text\n",
    "#                     reviewCommentList.append(reviewComment)\n",
    "# #                         print(reviewComment)\n",
    "#                 except:\n",
    "#                     pass\n",
    "\n",
    "\n",
    "#             # only until I fix the scroll\n",
    "#             reviewTitleList.pop()\n",
    "#             productTitleList.pop()\n",
    "#             reviewURLList.pop()\n",
    "\n",
    "#             data = {\n",
    "#                 'ProductTitle': productTitleList,\n",
    "#                 'ReviewURL': reviewURLList,\n",
    "#                 'ReviewTitle': reviewTitleList,\n",
    "#                 'ReviewStarRating': reviewRatingList,\n",
    "#                 'ReviewComment': reviewCommentList\n",
    "#             }\n",
    "#             print('stored data')\n",
    "#             productReviews_df = pd.DataFrame.from_dict(data)\n",
    "#             productReviews_df.head()\n",
    "#             print('into dataframe')\n",
    "\n",
    "#         except:\n",
    "#             pass\n",
    "    \n",
    "#         print('add to existing dataframe')\n",
    "#         review_df = review_df.append(productReviews_df, ignore_index=True)\n",
    "# print('done')\n",
    "# #                 browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(review_df)\n",
    "# review_df.tail()\n",
    "review_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = soup.find('div', class_='review-highlight')\n",
    "positive = reviews.find('div', class_='font-bold highlight-title').text\n",
    "stars = reviews.find('span', class_='seo-avg-rating').text\n",
    "# starts = reviews.find('span', class_='seo-average-rating')\n",
    "body = reviews.find('div', class_='collapsable-content-container').text\n",
    "print(positive)\n",
    "print(stars)\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = soup.find_all('div', class_='ReviewList-content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reviews \n",
    "\n",
    "review_df = pd.DataFrame()\n",
    "\n",
    "# for i in range(len(product_df[\"ReviewsURL\"])):\n",
    "for i in range(4,5):\n",
    "    if product_df['NumberofReviews'][i] > 0:\n",
    "#     number_of_reviews = product_df['NumberofReviews'][i]\n",
    "#     for j in range(number_of_reviews):\n",
    "        \n",
    "        productTitleList = []\n",
    "        reviewURLList = []\n",
    "        reviewSiteLink = []\n",
    "        reviewTitleList = []\n",
    "        reviewRatingList = []\n",
    "        reviewCommentList = []\n",
    "        \n",
    "#         if len(reviewRatingList) < product_df['NumberofReviews'][i]:\n",
    "        \n",
    "#         while len(reviewRatingList) < product_df['NumberofReviews'][i]:\n",
    "    \n",
    "        try:\n",
    "            # go to the product url link\n",
    "            browser.visit(product_df[\"ReviewsURL\"][i])\n",
    "            html = browser.html\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # find the 'see all reviews' button and go there\n",
    "            allReviewsURL = soup.find('a', class_=\"button ReviewBtn-container ReviewsHeader-seeAll button--primary\")['href']\n",
    "            browser.visit(base_url+allReviewsURL)\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # find the list of reviews\n",
    "            reviews = soup.find_all('div', class_='ReviewList-content')\n",
    "            \n",
    "#             for review in reviews:\n",
    "            for j in range(len(reviews)):\n",
    "                reviewSiteLink.append(review)\n",
    "                reviewSiteLink[j].send_keys(Keys.PAGE_DOWN)\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                reviews = soup.find('div', class_='ReviewList-content')\n",
    "                \n",
    "#                 print(f\"j={j}\")\n",
    "                productTitleList.append(product_df['Title'][i])\n",
    "                reviewURLList.append(product_df['ReviewsURL'][i])\n",
    "                title, stars, comments = scrapeReviews(review)\n",
    "                reviewTitleList.append(title)\n",
    "                reviewRatingList.append(stars)\n",
    "                reviewCommentList.append(comments)\n",
    "\n",
    "\n",
    "                data = {\n",
    "                    'ProductTitle': productTitleList,\n",
    "                    'ReviewURL': reviewURLList,\n",
    "                    'ReviewTitle': reviewTitleList,\n",
    "                    'ReviewStarRating': reviewRatingList,\n",
    "                    'ReviewComment': reviewCommentList\n",
    "                }\n",
    "                print('stored data')\n",
    "                productReviews_df = pd.DataFrame.from_dict(data)\n",
    "                productReviews_df.head()\n",
    "                print('into dataframe')\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        print('add to existing dataframe')\n",
    "        review_df = review_df.append(productReviews_df, ignore_index=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(review_df)\n",
    "# review_df.tail()\n",
    "review_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeReviews(review):\n",
    "    \n",
    "    # get the title of the review\n",
    "    try:\n",
    "        title = review.find('h3', class_='review-title').text\n",
    "    except:\n",
    "        title = 'None'\n",
    "        \n",
    "    # get the number of stars in this review\n",
    "    reviewRating = review.find('span', class_='seo-avg-rating').text\n",
    "    \n",
    "    # get the context of the review\n",
    "    reviewComment = review.find('p').text\n",
    "        \n",
    "    # return these items as a tuple\n",
    "    return title, stars, comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
